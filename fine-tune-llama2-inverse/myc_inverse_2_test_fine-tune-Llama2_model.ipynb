{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd67ba4-2468-4994-b5f8-1a201ce8ead0",
   "metadata": {},
   "source": [
    "## Notebook-2 Model Testing \n",
    "## Fine-tune Llama 2 for Inverse Information Generation \n",
    "(Reverse-Thinking) \n",
    "An instruction is a piece of text or prompt that is provided to an LLM to perform text generation of an answer. \n",
    ">\n",
    "***The goal is to create a model which can perform inverse information generation or filtering based on user input***. The idea behind this is that we want to the model able to filter large amount of information down into key points based on the fine-tune training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7a801-4a7c-478e-b579-c6cef1586267",
   "metadata": {},
   "source": [
    "### 1. Environment Setup and Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf3e2f-9c64-4f02-beee-e5a956522949",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1042468-c844-4c85-85dd-e9d28f8720e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:* cuda\n",
      "Device count: 1\n",
      "Device  Name: NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "   Allocated: 6.7 GB\n",
      "      Cached: 7.1 GB\n"
     ]
    }
   ],
   "source": [
    "def show_cuda_info():\n",
    "    import torch\n",
    "    # setting device on GPU if available, else CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:*', device)\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print('Device count:', torch.cuda.device_count())    \n",
    "        print('Device  Name:',torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('   Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('      Cached:', round(torch.cuda.torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "show_cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe60028-b5ba-4d12-8003-a01936a8293d",
   "metadata": {},
   "source": [
    "## Test-1 Use trained adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8c60d0-5d84-47da-8554-5bd1155add61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e505c88c3c46d8b7d4e24174fdb7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "train_model_output_dir = \"llama-7-int4-dolly\"\n",
    "# load base LLM model and tokenizer\n",
    "train_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    train_model_output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "train_tokenizer = AutoTokenizer.from_pretrained(train_model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0674fba1-4975-4249-b642-f5d1265ca0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "text_input=\"1. Mashing 2. Separation 3. Boiling 4. Fermentation. The ingredients are brought together through these 4 steps.\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{text_input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "input_ids = train_tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b142003-b67a-4cbf-bd99-5b0c5696264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pop/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated instruction:\n",
      "What are the 4 main steps in making beer?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = train_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "print(f\"Generated instruction:\\n{train_tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f06ecc-e075-4aee-b24a-83b4e2de74e3",
   "metadata": {},
   "source": [
    "Note: expected result: \"What are the 4 main steps in making beer?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648e2ce-75a8-4e33-9239-f4575b3a0136",
   "metadata": {},
   "source": [
    "## Test-2 Use merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d66270c-ac6c-4a2a-94a3-72f53614e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the Merged Model\n",
    "merged_model = \"merged_model\"\n",
    "model_id = merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9392061-43c8-4ad6-8397-23a774c750e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404224a2c82b432c865b8d356c45a596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e574d0c-7d18-45de-a541-8bdf78efb39a",
   "metadata": {},
   "source": [
    "##### Test Model and run Inference\n",
    "After the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79dc2a15-a29b-417d-9770-6f82b7cb5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ramdom question test \n",
    "##\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# # with torch.inference_mode():\n",
    "# outputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "# result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "# print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "# print(f\"**Generated instruction**:\\n{result}\")\n",
    "# #print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "# print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ad7430-a4a6-42b6-8d9d-7417790edaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"=== Original ===\")\n",
    "# print(\"Instructions:\\n\", sample[\"instruction\"])\n",
    "# # print(\"Context:\\n\", sample[\"context\"])\n",
    "# print(\"Response:\\n\",sample[\"response\"])\n",
    "# print(\"Category:\\n\",sample[\"category\"]) \n",
    "# print (\"=== Prompt ===\")\n",
    "# print(prompt)\n",
    "# print (\"=== Result ===\")\n",
    "# print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5547835b-e551-45cd-a73c-1fd5fccd42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input1=\"1. Mashing 2. Separation 3. Boiling 4. Fermentation. The ingredients are brought together through these 4 steps.\"\n",
    "text_input2='A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.'\n",
    "text_input3='Delta Lake is an open source storage layer that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fceeca-9f66-4199-98b1-f88cd81f3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73bc8e61-8442-45f6-94fa-4b061ace629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mashing 2. Separation 3. Boiling 4. Fermentation. The ingredients are brought together through these 4 steps.\n",
      "\n",
      "### Response:\n",
      "What are the 4 steps in the brewing process?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Direct input-1\n",
    "inputs = tokenizer(text_input1, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808f310-d823-4429-b530-b721b11dccd4",
   "metadata": {},
   "source": [
    "Note: expected result: \"What are the 4 main steps in making beer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a48d84a-8555-4e70-bf1f-a7d0cb33f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.\n",
      "\n",
      "### Response:\n",
      "What is a polygon?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Direct input-2\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text_input2, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bfd5f-de8c-4272-bb40-b5961b0c7665",
   "metadata": {},
   "source": [
    "Note: expected result: \"What is a polygon?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93197211-f910-4fce-bcc7-be07231e1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Lake is an open source storage layer that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs.\n",
      "\n",
      "### Response:\n",
      "What is Delta Lake?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Direct input-3\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text_input3, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833fda5-e45d-491b-9d93-130f8a620b81",
   "metadata": {},
   "source": [
    "Note: expected result: \"What is Delta Lake?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7405b-8e29-4f7c-b775-a105e3aecef4",
   "metadata": {},
   "source": [
    "## Test-3 Use huggingface transformer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdbd4139-5742-4436-89ae-5b38a4335bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd67e48-d9bd-42e1-9be0-812029fd972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1. Mashing 2. Separation 3. Boiling 4. Fermentation. The ingredients are brought together through these 4 steps.\n",
      "\n",
      "### Response:\n",
      "What are the 4 key steps in the brewing of beer?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## pipeline test input-1\n",
    "sequences = pipe(\n",
    "    text_input1,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed2b0b-63b1-4321-ba0b-bc53447e151b",
   "metadata": {},
   "source": [
    "Note: expected result: \"What are the 4 main steps in making beer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c36bf21-506f-46a4-b581-8a5d92d33394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.\n",
      "\n",
      "### Response:\n",
      "What is geometry?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## pipeline test input-2\n",
    "sequences = pipe(\n",
    "    text_input2,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646caf5-7fed-4063-8b95-a8aa3e4decfd",
   "metadata": {},
   "source": [
    "Note: expected result: \"What is a polygon?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2551c3c-5ada-433b-b22e-f5e49bc7c373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Delta Lake is an open source storage layer that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs. Delta Lake also provides an optimized query engine, the Delta Live Query engine, that can execute DML queries on your data lake with sub-second latency.\n",
      "\n",
      "Deltas are a powerful and intuitive way to manage change in your data lake, so it is natural to think about how you can make your data more like a delta.  This blog covers several ways to do this in Delta Lake and some important considerations when choosing among these options.\n",
      "\n",
      "### Response:\n",
      "What are ways you can make your data lake look more like a Delta?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## pipeline test input-3\n",
    "sequences = pipe(\n",
    "    text_input3,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006d4b0-53ef-4ebb-b4e3-da8e89c3ecf2",
   "metadata": {},
   "source": [
    "Note: expected result: \"What is Delta Lake?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4bdae-1d7a-4497-a253-df6958e70ec1",
   "metadata": {},
   "source": [
    "## Test-4 Langchain with system promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2ea8bb-0393-4cdb-a798-9cd62d77f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the Merged Model\n",
    "merged_model = \"merged_model\"\n",
    "model_id = merged_model\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0c2ff3-2650-44cd-808b-7ae6e9611686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading QLora model 4 or 8 bits from... merged_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bbb1cc2a4a46118eaf57fab1de178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer from... merged_model\n",
      "making a pipeline...\n",
      "setup llm chain...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,pipeline\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch\n",
    "\n",
    "qa_template = \"\"\"\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "{model_context}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "template = DEFAULT_SYSTEM_PROMPT+ qa_template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"model_context\",\"user_question\"])\n",
    "\n",
    "print('loading QLora model 4 or 8 bits from...', model_id)\n",
    "# BitsAndBytesConfig int-4 config\n",
    "# 4bits -- 8gb vram\n",
    "# 8bits -- 12gb vram\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    #load_in_4bit=True,\n",
    "    load_in_8bit=True,    \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "local_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\")\n",
    "\n",
    "print('loading tokenizer from...',model_id)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print('making a pipeline...')\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=local_model, \n",
    "    tokenizer=local_tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    model_kwargs={\"temperature\":0}\n",
    ")\n",
    "\n",
    "print('setup llm chain...')\n",
    "hf_llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf_llm, verbose=True)\n",
    "\n",
    "#print (\"==cuda after loading mode==\")\n",
    "#show_cuda_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5796175b-a977-40c7-b849-5dd1e48d3638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "\n",
      "Question: 1. Mashing 2. Separation 3. Boiling 4. Fermentation. The ingredients are brought together through these 4 steps.\n",
      "\n",
      "\n",
      "\n",
      "Answer: Let's think step by step.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pop/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text=\"1. Mashing 2. Separation 3. Boiling 4. Fermentation. The ingredients are brought together through these 4 steps.\"\n",
    "#input_text='A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.'\n",
    "input_context=\"\"\n",
    "local_result=llm_chain.run({'model_context': input_context,\n",
    "                            'user_question':input_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43fbadd-3557-4d8d-89bc-618f3735c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==ANSWER==\n",
      "\n",
      "\n",
      "1. Mashing: The ingredients are mixed together to form a mash.\n",
      "2. Separation: The mash is separated into liquid and solid.\n",
      "3. Boiling: The liquid is boiled to evaporate the water.\n",
      "4. Fermentation: The liquid is fermented to produce alcohol.\n",
      "\n",
      "### Response:\n",
      "What are the steps in making beer?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==ANSWER==\")\n",
    "print(local_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ae1a2-2b4f-4e8c-b360-1e4116006224",
   "metadata": {},
   "source": [
    "Note: expected result: \"What are the 4 main steps in making beer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7637666-f5d9-4cd8-a0cd-0e18eb2a0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==cuda after text generation==\n",
      "Using device:* cuda\n",
      "Device count: 1\n",
      "Device  Name: NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "   Allocated: 6.7 GB\n",
      "      Cached: 7.1 GB\n"
     ]
    }
   ],
   "source": [
    "print (\"==cuda after text generation==\")\n",
    "show_cuda_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e77feae-ad90-4721-a0e2-7e268ee62911",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Testing Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunellama2",
   "language": "python",
   "name": "tunellama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
