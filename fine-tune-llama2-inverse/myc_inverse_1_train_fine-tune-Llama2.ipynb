{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d27b950-e1fe-4cd9-a844-6c2956041960",
   "metadata": {},
   "source": [
    "## Notebook-1 \n",
    "## Fine-tune Llama 2 for Inverse Information Generation \n",
    "(Reverse-Thinking) \n",
    "An instruction is a piece of text or prompt that is provided to an LLM to perform text generation of an answer. \n",
    ">\n",
    "***The goal is to create a model which can perform inverse information generation or filtering based on user input***. The idea behind this is that we want to the model able to filter large amount of information down into key points based on the fine-tune training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42206b7d-8712-4cfe-a265-e654924f37da",
   "metadata": {},
   "source": [
    "### 1. Environment Setup and Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbd20a-890e-45a2-ad87-06e96c65cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "650312b5-3455-43db-b669-21313879e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:* cuda\n",
      "Device count: 1\n",
      "Device  Name: NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "   Allocated: 4.1 GB\n",
      "      Cached: 4.4 GB\n"
     ]
    }
   ],
   "source": [
    "def show_cuda_info():\n",
    "    import torch\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:*', device)\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print('Device count:', torch.cuda.device_count())    \n",
    "        print('Device  Name:',torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('   Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('      Cached:', round(torch.cuda.torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "show_cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38f1a7-c3c2-46ef-b074-bd31def8e2f4",
   "metadata": {},
   "source": [
    "#### install dependencies\n",
    "this steps may take more than 10 minutes to complete to compile flasj-attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64463e9a-a3f7-4e08-bcb5-efa76444cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /home/pop/.local/lib/python3.10/site-packages (1.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (23.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: flash-attn in /opt/conda/envs/tunellama2/lib/python3.10/site-packages/flash_attn-2.0.4-py3.10-linux-x86_64.egg (2.0.4)\n",
      "Requirement already satisfied: torch in /home/pop/.local/lib/python3.10/site-packages (from flash-attn) (2.0.1)\n",
      "Requirement already satisfied: einops in /home/pop/.local/lib/python3.10/site-packages (from flash-attn) (0.6.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from flash-attn) (23.1)\n",
      "Requirement already satisfied: ninja in /home/pop/.local/lib/python3.10/site-packages (from flash-attn) (1.11.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from torch->flash-attn) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/pop/.local/lib/python3.10/site-packages (from torch->flash-attn) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/pop/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->flash-attn) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/pop/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->flash-attn) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/tunellama2/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja packaging\n",
    "!export MAX_JOBS=3 \n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9619c8-e5ab-40b3-b70a-bcc9db10acf8",
   "metadata": {},
   "source": [
    "### 2.Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a11bee80-63bc-4455-b568-e651f1c6beb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d8de6e87b4051b433b110e9bb419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# required for download dataset from Huggingface\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71828880-8a4d-47da-8eab-fff30f3de8a6",
   "metadata": {},
   "source": [
    "To load the databricks/databricks-dolly-15k dataset, we use the load_dataset() method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9249c7e-b612-4e0f-afd7-2228633ae69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/pop/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n"
     ]
    }
   ],
   "source": [
    "## load from hub\n",
    "from datasets import load_dataset\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "print(f\"dataset size: {len(dolly_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a625630-cfa0-41e3-8cc8-35ad7094c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "{sample['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2f62476-43b7-4829-8833-6da6a7ebfdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Why can camels survive for long without water?', 'context': '', 'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "## spot check origin sample record\n",
    "print(dolly_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c0bdf8c-3114-40ee-acef-d0c0940a412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
      "\n",
      "### Input:\n",
      "Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.\n",
      "\n",
      "### Response:\n",
      "Why can camels survive for long without water?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sample formatted training instruction\n",
    "print(format_training_instruction(dolly_dataset[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3348958-12d0-4b70-9fd9-094708414d71",
   "metadata": {},
   "source": [
    "## 3 Instruction-tune Llama 2 using trl and the SFTTrainer\n",
    "##### how QLoRA works is:\n",
    "\n",
    "    Quantize the pre-trained model to 4 bits and freeze it.\n",
    "    Attach small, trainable adapter layers. (LoRA)\n",
    "    Finetune only the adapter layers while using the frozen quantized model for context.\n",
    "\n",
    "If you want to learn more about QLoRA and how it works, I recommend you to read the Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA blog post.\n",
    "\n",
    "**Flash Attention** is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper.  The TL;DR; accelerates training up to 3x. **Big cost saving feature when renting environment in Cloud**.\n",
    "\n",
    "However, due hardware limitation, will not be use on this notebook because Flash attention limited support on specific NVIDIA card like A,H series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb5c1e-2aaf-43d2-98d1-796456649b80",
   "metadata": {},
   "source": [
    "#### Flash Attention  \n",
    "\n",
    "Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper.  The TL;DR; accelerates training up to 3x.\n",
    "\n",
    "Note1: Big cost saving feature when renting environment in Cloud\n",
    "\n",
    "Note2: Flash attention limited support on specific NVIDIA card like A,H series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9829ad-eba4-4266-9c85-8e1c434e72ae",
   "metadata": {},
   "source": [
    "#### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7d2f6a0-f69f-477d-985f-f4697d4071db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4c9ff7c3f54acb8d16d603670e44c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# Select Hugging Face model id gated or non-gated\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d95aaa-e9a6-4f47-82e6-ab48344bda2f",
   "metadata": {},
   "source": [
    "#### PERF Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9170c-d292-4d7d-9f25-48891dfdc8ea",
   "metadata": {},
   "source": [
    "The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e844216-6dac-4e25-874a-aa27cba623b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983137e-8c64-40ae-8715-bfe3dda4258e",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5cf990d-054e-44f7-a115-79999577a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7-int4-dolly\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dfe67-fef5-490f-944c-559191ca191c",
   "metadata": {},
   "source": [
    "#### Create SFTTrainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb63df09-103e-4f90-8e9b-d155f9297feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 ms, sys: 810 Âµs, total: 3.57 ms\n",
      "Wall time: 5.71 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# max sequence length for model and packing of the dataset\n",
    "max_seq_length = 2048 \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dolly_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_training_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e274c-bfde-48f4-b7dc-ed48bc467615",
   "metadata": {},
   "source": [
    "#### Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f6a2e3-198a-403d-b5bf-0a8a6dfc6508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmychen76\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pop/mclab-ai/kx_tuning/wandb/run-20230812_011245-fenmwsb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mychen76/huggingface/runs/fenmwsb4' target=\"_blank\">revived-shape-9</a></strong> to <a href='https://wandb.ai/mychen76/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mychen76/huggingface' target=\"_blank\">https://wandb.ai/mychen76/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mychen76/huggingface/runs/fenmwsb4' target=\"_blank\">https://wandb.ai/mychen76/huggingface/runs/fenmwsb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6449, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.388, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.3566, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.2871, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.29, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.2353, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.2358, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.2327, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.2411, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.2298, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.2336, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.2789, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.1953, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.198, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.2051, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 1.2241, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
      "{'loss': 1.2001, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
      "{'loss': 1.2015, 'learning_rate': 0.0002, 'epoch': 1.02}\n",
      "{'loss': 1.1949, 'learning_rate': 0.0002, 'epoch': 1.03}\n",
      "{'loss': 1.2752, 'learning_rate': 0.0002, 'epoch': 1.03}\n",
      "{'loss': 1.2199, 'learning_rate': 0.0002, 'epoch': 1.04}\n",
      "{'loss': 1.2329, 'learning_rate': 0.0002, 'epoch': 1.04}\n",
      "{'loss': 1.2151, 'learning_rate': 0.0002, 'epoch': 1.05}\n",
      "{'loss': 1.2043, 'learning_rate': 0.0002, 'epoch': 1.05}\n",
      "{'loss': 1.2337, 'learning_rate': 0.0002, 'epoch': 1.06}\n",
      "{'loss': 1.2279, 'learning_rate': 0.0002, 'epoch': 1.06}\n",
      "{'loss': 1.1683, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
      "{'loss': 1.1687, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
      "{'loss': 1.1881, 'learning_rate': 0.0002, 'epoch': 2.0}\n",
      "{'loss': 1.2327, 'learning_rate': 0.0002, 'epoch': 2.01}\n",
      "{'loss': 1.2088, 'learning_rate': 0.0002, 'epoch': 2.01}\n",
      "{'loss': 1.1975, 'learning_rate': 0.0002, 'epoch': 2.02}\n",
      "{'loss': 1.2077, 'learning_rate': 0.0002, 'epoch': 2.02}\n",
      "{'loss': 1.2093, 'learning_rate': 0.0002, 'epoch': 2.03}\n",
      "{'loss': 1.207, 'learning_rate': 0.0002, 'epoch': 2.03}\n",
      "{'loss': 1.1934, 'learning_rate': 0.0002, 'epoch': 2.04}\n",
      "{'loss': 1.1977, 'learning_rate': 0.0002, 'epoch': 2.05}\n",
      "{'loss': 1.1985, 'learning_rate': 0.0002, 'epoch': 2.05}\n",
      "{'loss': 1.1955, 'learning_rate': 0.0002, 'epoch': 2.06}\n",
      "{'loss': 1.1505, 'learning_rate': 0.0002, 'epoch': 2.06}\n",
      "{'loss': 1.1639, 'learning_rate': 0.0002, 'epoch': 2.07}\n",
      "{'loss': 1.1368, 'learning_rate': 0.0002, 'epoch': 2.07}\n",
      "{'train_runtime': 8208.2839, 'train_samples_per_second': 5.486, 'train_steps_per_second': 0.686, 'train_loss': 1.2315216935490556, 'epoch': 2.08}\n",
      "CPU times: user 1h 10min 14s, sys: 1h 6min 45s, total: 2h 16min 59s\n",
      "Wall time: 2h 16min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "trainer.train() \n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1266a30-23e2-46e6-b4f2-0aab25b749af",
   "metadata": {},
   "source": [
    "### Load Trained LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999a0195-c7a4-4625-b7a9-207759f6b211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6230494fb760413ebb5fd701e51d4bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "args.output_dir = \"llama-7-int4-dolly\"\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241dd17-3a74-4a48-b80d-eefeb3ed64a4",
   "metadata": {},
   "source": [
    "### Merged and Save Model\n",
    "After the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2945896-bcde-4c96-a4fb-e9d9a435ddb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbb65291c3143a0878d6762fff50e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.model',\n",
       " 'merged_model/added_tokens.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "\n",
    "# push merged model to the hub\n",
    "# merged_model.push_to_hub(\"user/repo\")\n",
    "# tokenizer.push_to_hub(\"user/repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4c5238a-2bcc-4ec1-9be7-92f3f7942a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35efcea-f76e-40e0-a08d-24e20b7dbc47",
   "metadata": {},
   "source": [
    "### see notebook-2 for model testing and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98a1e9-56e2-4731-9d2d-70fa22b0869d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunellama2",
   "language": "python",
   "name": "tunellama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
